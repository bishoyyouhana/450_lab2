{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataloader' from '/proj/ece450-spring2022/bby223/Lab2_Release_0303/dataloader.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You cannot change this line.\n",
    "from dataloader import CIFAR10\n",
    "import dataloader\n",
    "import imp\n",
    "imp.reload(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################  FIRST ATTEMPT   ###########################################################\n",
    "# ##### until 35 epochs it gives us 85 percent with lr =0.01, sgd, no transforms,\n",
    "# ##### dropouts of 0.5 prove to be too much \n",
    "\n",
    "# class model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(model, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         #pool 2,2\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         #pool 2,2\n",
    "#         self.bn2 = nn.BatchNorm2d(128)\n",
    "#         self.drop2 = nn.Dropout2d(p=0.05)\n",
    "        \n",
    "#         self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         #pool 2,2\n",
    "#         self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "#         self.flat = nn.Flatten()\n",
    "        \n",
    "#         self.fc1 =nn.Linear(256*4*4, 1024)\n",
    "#         #relu\n",
    "#         self.drop1 = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         #relu\n",
    "#         self.drop3 = nn.Dropout(p=0.1)\n",
    "#         self.fc3 = nn.Linear(512, 10)\n",
    "#         #relu\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.conv1(x))\n",
    "#         out = F.relu(self.conv2(out))\n",
    "#         out = self.bn1(F.max_pool2d(out, 2))\n",
    "        \n",
    "#         out = F.relu(self.conv3(out))\n",
    "#         out = F.relu(self.conv4(out))\n",
    "#         out = self.bn2(F.max_pool2d(out, 2))\n",
    "#         out = self.drop2(out)\n",
    "        \n",
    "#         out = F.relu(self.conv5(out))\n",
    "#         out = F.relu(self.conv6(out))\n",
    "#         out = self.bn3(F.max_pool2d(out, 2))\n",
    "        \n",
    "#         out = self.flat(out)\n",
    "\n",
    "# #         out = out.view(out.size(0), -1)\n",
    "        \n",
    "#         out = F.relu(self.fc1(out))\n",
    "#         out = self.drop1(out)\n",
    "#         out = F.relu(self.fc2(out))\n",
    "# #         out = self.drop3(out)\n",
    "#         out = F.relu(self.fc3(out))\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################  SECOND ATTEMPT   ###########################################################\n",
    "\n",
    "# ##### until 35 epochs it gives us 85 percent with lr =0.01, sgd, no transforms,\n",
    "# ##### dropouts of 0.5 prove to be too much \n",
    "\n",
    "# ##### dropouts 0.5 and 0.1, lr = 0.1, 100 epochs. sgd, 87 percent\n",
    "# class model(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(model, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         #pool 2,2\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         #pool 2,2\n",
    "#         self.bn2 = nn.BatchNorm2d(128)\n",
    "#         self.drop2 = nn.Dropout2d(p=0.05)\n",
    "        \n",
    "#         self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         #pool 2,2\n",
    "#         self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "#         self.conv7 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         self.conv8 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "#         #relu\n",
    "#         #pool 2,2\n",
    "#         self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "#         self.flat = nn.Flatten()\n",
    "        \n",
    "#         self.fc1 =nn.Linear(512*2*2, 1024)\n",
    "#         #relu\n",
    "#         self.drop1 = nn.Dropout(p=0.5)\n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         #relu\n",
    "#         self.drop3 = nn.Dropout(p=0.1)\n",
    "#         self.fc3 = nn.Linear(512, 10)\n",
    "#         #relu\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = F.relu(self.conv1(x))\n",
    "#         out = F.relu(self.conv2(out))\n",
    "#         out = self.bn1(F.max_pool2d(out, 2))\n",
    "        \n",
    "#         out = F.relu(self.conv3(out))\n",
    "#         out = F.relu(self.conv4(out))\n",
    "#         out = self.bn2(F.max_pool2d(out, 2))\n",
    "#         out = self.drop2(out)\n",
    "        \n",
    "#         out = F.relu(self.conv5(out))\n",
    "#         out = F.relu(self.conv6(out))\n",
    "#         out = self.bn3(F.max_pool2d(out, 2))\n",
    "        \n",
    "#         out = F.relu(self.conv7(out))\n",
    "#         out = F.relu(self.conv8(out))\n",
    "#         out = self.bn4(F.max_pool2d(out, 2))\n",
    "        \n",
    "#         out = self.flat(out)\n",
    "\n",
    "# #         out = out.view(out.size(0), -1)\n",
    "        \n",
    "#         out = F.relu(self.fc1(out))\n",
    "#         out = self.drop1(out)\n",
    "#         out = F.relu(self.fc2(out))\n",
    "# #         out = self.drop3(out)\n",
    "#         out = F.relu(self.fc3(out))\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################  FINAL ATTEMPT   ###########################################################\n",
    "\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.neuralnet = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "#             nn.Dropout2d(p=0.05),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "    \n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), \n",
    "            nn.Dropout2d(p=0.05),\n",
    "            \n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512*2*2, 1024),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout2d(p=0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.neuralnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 30\n",
    "INITIAL_LR = 0.05\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-5\n",
    "EPOCHS = 130\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference value for mean/std:\n",
    "\n",
    "mean(RGB-format): (0.4914, 0.4822, 0.4465)\n",
    "std(RGB-format): (0.2023, 0.1994, 0.2010)\n",
    "\"\"\"\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "#      transforms.CenterCrop(10),\n",
    "        transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    \n",
    "    ])\n",
    "\n",
    "transform_val = transforms.Compose(\n",
    "    [\n",
    "transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Call the dataset Loader\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, test=False,download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "valset = CIFAR10(root=DATAROOT, train=False,test=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader for testing process\n",
    "testset = CIFAR10(root=\"\", train=False, test=True, transform=transform_val)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "net = model()\n",
    "net = net.to(device)\n",
    "print(device)\n",
    "if device =='cuda:1':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint: ./saved_model/model.h5\n",
      "Starting from epoch 115 \n",
      "Starting from learning rate 0.002174:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=INITIAL_LR, momentum=MOMENTUM, weight_decay=REG)\n",
    "\n",
    "# adam proved to be weaker\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(),lr=INITIAL_LR,weight_decay=REG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-19 23:55:25.170044\n",
      "Epoch 115:\n",
      "45000\n",
      "Training loss: 0.0219, Training accuracy: 0.9927\n",
      "2022-03-19 23:55:35.756310\n",
      "Validation...\n",
      "Validation loss: 0.4616, Validation accuracy: 0.9106\n",
      "Current learning rate has decayed to 0.002083\n",
      "Saving ...\n",
      "2022-03-19 23:55:37.151148\n",
      "Epoch 116:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-529a43d2abf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Apply gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Calculate predicted labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/2019.10/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start the training/validation process\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        \n",
    "#         print(len(inputs))\n",
    "#         print(len(len(inputs)))\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.requires_grad_().to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "#         print(inputs)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += targets.size(0)\n",
    "        correct_examples += (predicted == targets).sum().item()\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(total_examples)\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "#             print(\"----------------------------------------------------------------------------------\")\n",
    "#             print(inputs)\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.requires_grad_().to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Calculate accuracy\n",
    "            total_examples +=targets.size(0)\n",
    "            correct_examples += (predicted == targets).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    \n",
    "\n",
    "    DECAY_EPOCHS = 5\n",
    "    DECAY = 1.00\n",
    "    \n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "#         current_learning_rate = (pow(0.95,(i/DECAY_EPOCHS)))*INITIAL_LR\n",
    "        current_learning_rate = (1/(1+(i/DECAY_EPOCHS)))*INITIAL_LR\n",
    "#         current_learning_rate = (0.95^(i/DECAY_EPOCHS)*INITIAL_LR\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            torch.optim.SGD(net.parameters(), lr=current_learning_rate, momentum=MOMENTUM, weight_decay=REG)\n",
    "            \n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create Kaggle compatible soltuions\n",
    "import csv\n",
    "\n",
    "# open the file in the write mode\n",
    "f = open('soltuion.csv', 'w')\n",
    "writer = csv.writer(f)\n",
    "\n",
    "with torch.no_grad():\n",
    "#     print(enumerate(testloader))\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "#         print(inputs)\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.requires_grad_().to(device)\n",
    "#         targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output from the DNN.\n",
    "        outputs = net(inputs)\n",
    "#         loss = criterion(outputs, targets)\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        # Calculate accuracy\n",
    "        #total_examples +=targets.size(0)\n",
    "        #correct_examples += (predicted == targets).sum().item()\n",
    "#         val_loss += loss\n",
    "#         predictions += predicted\n",
    "        writer.writerow([batch_idx, predicted.item()])\n",
    "        print(predicted.item())\n",
    "\n",
    "f.close()\n",
    "# avg_loss = val_loss / len(valloader)\n",
    "# avg_acc = correct_examples / total_examples\n",
    "# print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
